The objective of this assignment is to compare the performance of different classifiers, and draw actionable recommendations from the models. The data being leveraged is from a portuguese banking system, and demonstrates features of customers being contacted, including personal and financial features and their history with prior marketing campaigns. As the target varible, the dataset shows whether the customer converted or not. 
Mass advertising and marketing is becoming less and less common given the low return on the massive investment and the frustration it brings to irrelevant customers. Target advertising and precision marketing is a method in which businesses identify the customers most likely to convert and focus their marketing budget on that subset. Classification is one of the methods that empowers businesses to identify this subset. This can be achieved by training a classification model on historical data, identifying features that are most relevant in defining the outcome (or the class), and leveraging the model to determine whether a new customer will convert or not.
We started this assignment by building a baseline Model for this data. This was done but considering the accuracy of a fake model that always returns the most common outcome of the target variable, which in this case is 'no'. This baseline with accuracy of 88.7% served as a benchmark where we could compare the accuracy of the more sophisticated models to ensure they outperform the dummy model.
This excercise included building and comparing the results of 4 classification models: KNN, Logistic Regression, Decision Tree and SVM.
To being, as a secondary baseline, we built the most simple version of each model with default settings on a reduced set of features, and compared their accuracies and run time. The test set accuracy scores are as follows: KNN: 87.7%, Logistic Regression: 88.7%, Decision Tree: 93.4%, SVM: 88.7%. 
The next step was to do feature engineeing to reduce the total count of features and to perform grid search on the hyperparameters of each model to find the best version that exceeds the baseline of the default setting models.
While we were able to find models performing better than the baselines in all cases, it is important to note that 'accuracy' is not the only measures of success in classification models, spcecially for models that have imbalanced set of target variables. Thus, for every model, we looked at the precision/recall score associated with different values of threshold as well. The correct tradeoff depends on the business case, and for a marketing campaign, cost of a false pasitive is not high yet a false negative in a dataset with 88.7% chance of negative outcome would be a significant missed opportunity. Thus, having an optimistic classifier with low Threshold was the correct choice.
In case of KNN,  the optimal classifier had 'n_neighbors' = 29. While the standard T= 0.5 had accuracy of 0.90 which exceeded the baseline models, due to the high count of false negatives which led to recall score of 0.23, we suggested threshold of T=0.3 which would keep accuracy at 0.89 but would double the recall score to 0.45.
In case of Logistic Regression, we iterated over L1 and L2 regularization with different weights for C. The optimal result was L1 regularization with C= 0.01. Similar to KNN, the while the accuracy exceeded the baseline model at 0.90, there were high counts of false negatives with recall score at 0.20. Thus, again the recommended threshold is T =0.3, which would bring the accuracy to 0.89 while doubling the recall score to 0.41. The L1 regularization also sets the coefficients to zero for features that don't contribute to the classification. This enables us to better understanding the facors that lead to a negative or positive classification: 
The feature most associated with conversion are: 1) month of March, 2) outcome of previous marketing camptaign, 3) job being retired or a student. Features most likely to lead to a negative response are: 1) number of employees being large, 2) euribor3m, 3) outcome of previous marketing campaign being negative, 4) day of the week being Monday, 5) Having a blue collar or service job.
For Decision Tree, we went through the same grid search exercise and got max_depth of 4 and min_sample_split of 0.01. Again, to have better recall score, we recommended having T= 0.3 that with an accuracy of 0.88 would give a recall score of 0.53.
For SVM, given how computationally expensive the model is, we took a sample that was 10% of the original data size. In addition, we didn't include polynomial kernel functions in the grid search because they were to computationally expensive. The resulting watered down model didn't perform well with recall score. 
Conclusion: SVM certainly is powerful but is computationally expensive. Between KNN, Linear Regression and Decision Tree, when looking at the Accuracy in conjunction with Recall - which is critical with the imbalance in our target variable - Decision Tree returned the best results. Decision Tree is computationally for expensive than KNN for fitting the data, however, for predicing the class for new datapoints works better than KNN. Note: after identifying the most important features with L1 regularization of my Logistic Regression, I applied KNN to those 7 features only and obtained results that were on-par with my Decision Tree. Thus, I would use both the Decision Tree and the KNN with reduced features to make predictions in future.
Recommendation: besides the model recommendation made in the last step, the business recommendation is for associates to follow the most important features that were derived from the logist regression regularization (see step 10) to increase the conversation rates. For example, be aware that chances of converting is higher in March, with customers who responded well to other marketing campaings and are either student or retired. Don't spend too much time on the leads that meet the negative coefficients listed in step 10. We can also look into the feature_importance_ in the Decision Trees, but while those features are important in how the branches are formed, they don't help with predicting what the likely outcome is. Other than looking at the features, associates can use the Decision Tree model with the optimal parameters to see if the lead they're pursuing has a chance to convert or not.
